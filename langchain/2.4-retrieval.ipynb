{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Retrieval\n",
    "\n",
    "Retrieval is the centerpiece of our retrieval augmented generation (RAG) flow. \n",
    "\n",
    "Let's get our vectorDB from before."
   ],
   "id": "c92fb8f010c88f46"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "%pip install python-dotenv langchain langchain-openai chromadb docarray --upgrade --quiet",
   "id": "1477803c123bac8c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Vectorstore retrieval\n",
   "id": "2df7b7494e0a856b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T06:35:59.690086Z",
     "start_time": "2024-04-11T06:35:59.214335Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#import os\n",
    "#import openai\n",
    "#import sys\n",
    "#sys.path.append('../..')\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "#openai.api_key  = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "#!pip install lark"
   ],
   "id": "e888a8dd3ca3da",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Similarity Search",
   "id": "2b8a7bb26c903db6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T06:36:00.389420Z",
     "start_time": "2024-04-11T06:35:59.691142Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "persist_directory = '../db/chroma-step2.3'\n",
    "\n",
    "embedding = OpenAIEmbeddings()\n",
    "vectordb = Chroma(\n",
    "    persist_directory=persist_directory,\n",
    "    embedding_function=embedding\n",
    ")\n",
    "\n",
    "print(vectordb._collection.count())"
   ],
   "id": "7ffe6161f24cfcf5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "209\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T06:36:01.319718Z",
     "start_time": "2024-04-11T06:36:00.390111Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "texts = [\n",
    "    \"\"\"The Amanita phalloides has a large and imposing epigeous (aboveground) fruiting body (basidiocarp).\"\"\",\n",
    "    \"\"\"A mushroom with a large fruiting body is the Amanita phalloides. Some varieties are all-white.\"\"\",\n",
    "    \"\"\"A. phalloides, a.k.a Death Cap, is one of the most poisonous of all known mushrooms.\"\"\",\n",
    "]\n",
    "\n",
    "smalldb = Chroma.from_texts(texts, embedding=embedding)"
   ],
   "id": "5dcb485c68ce4b8e",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T06:36:01.832917Z",
     "start_time": "2024-04-11T06:36:01.320621Z"
    }
   },
   "cell_type": "code",
   "source": [
    "question = \"Tell me about all-white mushrooms with large fruiting bodies\"\n",
    "\n",
    "smalldb.similarity_search(question, k=2)"
   ],
   "id": "7cfadac4ea9d6bd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='A mushroom with a large fruiting body is the Amanita phalloides. Some varieties are all-white.'),\n",
       " Document(page_content='The Amanita phalloides has a large and imposing epigeous (aboveground) fruiting body (basidiocarp).')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Try again with MMR",
   "id": "fd88f563f59a28bc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T06:36:02.037082Z",
     "start_time": "2024-04-11T06:36:01.835112Z"
    }
   },
   "cell_type": "code",
   "source": "smalldb.max_marginal_relevance_search(question,k=2, fetch_k=3)",
   "id": "fd8617db8fa5733a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='A mushroom with a large fruiting body is the Amanita phalloides. Some varieties are all-white.'),\n",
       " Document(page_content='A. phalloides, a.k.a Death Cap, is one of the most poisonous of all known mushrooms.')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T06:36:02.040035Z",
     "start_time": "2024-04-11T06:36:02.038265Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "5991f791b3ba361c",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Addressing Diversity: Maximum marginal relevance\n",
    "\n",
    "Last class we introduced one problem: how to enforce diversity in the search results.\n",
    " \n",
    "`Maximum marginal relevance` strives to achieve both relevance to the query *and diversity* among the results."
   ],
   "id": "15922aa064cd4229"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T06:36:02.415234Z",
     "start_time": "2024-04-11T06:36:02.040897Z"
    }
   },
   "cell_type": "code",
   "source": [
    "question = \"what did they say about matlab?\"\n",
    "docs_ss = vectordb.similarity_search(question,k=3)"
   ],
   "id": "dacc8fe91f837843",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T06:36:02.418435Z",
     "start_time": "2024-04-11T06:36:02.416041Z"
    }
   },
   "cell_type": "code",
   "source": "docs_ss[0].page_content[:100]",
   "id": "405d02b12fa40dd2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'those homeworks will be done in either MATLA B or in Octave, which is sort of — I \\nknow some people '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T06:36:02.420957Z",
     "start_time": "2024-04-11T06:36:02.419079Z"
    }
   },
   "cell_type": "code",
   "source": "docs_ss[1].page_content[:100]",
   "id": "a4bdf0e8b6353cb9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'those homeworks will be done in either MATLA B or in Octave, which is sort of — I \\nknow some people '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Note the difference in results with `MMR`.",
   "id": "5bcfcad5c19d886b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T06:36:02.781168Z",
     "start_time": "2024-04-11T06:36:02.421508Z"
    }
   },
   "cell_type": "code",
   "source": "docs_mmr = vectordb.max_marginal_relevance_search(question,k=3)",
   "id": "41ef47c5eebf8be5",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T06:36:02.800902Z",
     "start_time": "2024-04-11T06:36:02.788374Z"
    }
   },
   "cell_type": "code",
   "source": "docs_mmr[0].page_content[:100]",
   "id": "7c82de8cdec89d41",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'those homeworks will be done in either MATLA B or in Octave, which is sort of — I \\nknow some people '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T06:36:02.813573Z",
     "start_time": "2024-04-11T06:36:02.805920Z"
    }
   },
   "cell_type": "code",
   "source": "docs_mmr[1].page_content[:100]",
   "id": "58089caf79a124e2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'algorithm then? So what’s different? How come  I was making all that noise earlier about \\nleast squa'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Addressing Specificity: working with metadata\n",
    "\n",
    "In last lecture, we showed that a question about the third lecture can include results from other lectures as well.\n",
    "\n",
    "To address this, many vectorstores support operations on `metadata`.\n",
    "\n",
    "`metadata` provides context for each embedded chunk."
   ],
   "id": "21dbaeda1c0ce6e9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T06:36:02.823503Z",
     "start_time": "2024-04-11T06:36:02.817029Z"
    }
   },
   "cell_type": "code",
   "source": "question = \"what did they say about regression in the third lecture?\"",
   "id": "fe4f94810b106511",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T06:36:03.081921Z",
     "start_time": "2024-04-11T06:36:02.831158Z"
    }
   },
   "cell_type": "code",
   "source": [
    "docs = vectordb.similarity_search(\n",
    "    question,\n",
    "    k=3,\n",
    "    filter={\"source\":\"../data/MachineLearning-Lecture03.pdf\"}\n",
    ")"
   ],
   "id": "3b402bbc62917ce9",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T06:36:03.084542Z",
     "start_time": "2024-04-11T06:36:03.082531Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for d in docs:\n",
    "    print(d.metadata)"
   ],
   "id": "45cffeb670b21acf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'page': 0, 'source': '../data/MachineLearning-Lecture03.pdf'}\n",
      "{'page': 14, 'source': '../data/MachineLearning-Lecture03.pdf'}\n",
      "{'page': 4, 'source': '../data/MachineLearning-Lecture03.pdf'}\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Addressing Specificity: working with metadata using self-query retriever\n",
    "\n",
    "But we have an interesting challenge: we often want to infer the metadata from the query itself.\n",
    "\n",
    "To address this, we can use `SelfQueryRetriever`, which uses an LLM to extract:\n",
    " \n",
    "1. The `query` string to use for vector search\n",
    "2. A metadata filter to pass in as well\n",
    "\n",
    "Most vector databases support metadata filters, so this doesn't require any new databases or indexes."
   ],
   "id": "6ce470b539e65583"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T06:36:03.502549Z",
     "start_time": "2024-04-11T06:36:03.085294Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_openai import OpenAI\n",
    "#from langchain.llms import OpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain.chains.query_constructor.base import AttributeInfo"
   ],
   "id": "9f41d5bc07fc9b8f",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T06:36:03.505Z",
     "start_time": "2024-04-11T06:36:03.503290Z"
    }
   },
   "cell_type": "code",
   "source": [
    "metadata_field_info = [\n",
    "    AttributeInfo(\n",
    "        name=\"source\",\n",
    "        description=\"The lecture the chunk is from, should be one of `../data/MachineLearning-Lecture01.pdf`, `../data/MachineLearning-Lecture02.pdf`, or `../data/MachineLearning-Lecture03.pdf`\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"page\",\n",
    "        description=\"The page from the lecture\",\n",
    "        type=\"integer\",\n",
    "    ),\n",
    "]"
   ],
   "id": "53bbfae607afb1aa",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Note:** The default model for `OpenAI` (\"from langchain.llms import OpenAI\") is `text-davinci-003`. Due to the deprication of OpenAI's model `text-davinci-003` on 4 January 2024, you'll be using OpenAI's recommended replacement model `gpt-3.5-turbo-instruct` instead.",
   "id": "925f8a725a890449"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T06:36:03.535095Z",
     "start_time": "2024-04-11T06:36:03.505720Z"
    }
   },
   "cell_type": "code",
   "source": [
    "document_content_description = \"Lecture notes\"\n",
    "#llm = OpenAI(model='gpt-3.5-turbo-instruct', temperature=0)\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "retriever = SelfQueryRetriever.from_llm(\n",
    "    llm,\n",
    "    vectordb,\n",
    "    document_content_description,\n",
    "    metadata_field_info,\n",
    "    verbose=True\n",
    ")"
   ],
   "id": "fedc9dce12840ce1",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T06:36:03.537454Z",
     "start_time": "2024-04-11T06:36:03.535848Z"
    }
   },
   "cell_type": "code",
   "source": "question = \"what did they say about regression in the third lecture?\"",
   "id": "fb3b8b4d6dfbdcae",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**You will receive a warning** about predict_and_parse being deprecated the first time you executing the next line. This can be safely ignored.",
   "id": "cb77bb80a153d581"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T06:36:05.043301Z",
     "start_time": "2024-04-11T06:36:03.538172Z"
    }
   },
   "cell_type": "code",
   "source": [
    "docs = retriever.invoke(question)\n",
    "print(len(docs))"
   ],
   "id": "12c1be57d1cecd20",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T06:36:05.045966Z",
     "start_time": "2024-04-11T06:36:05.044085Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for d in docs:\n",
    "    print(d.metadata)"
   ],
   "id": "efe3621483f7c777",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'page': 14, 'source': '../data/MachineLearning-Lecture03.pdf'}\n",
      "{'page': 10, 'source': '../data/MachineLearning-Lecture03.pdf'}\n",
      "{'page': 0, 'source': '../data/MachineLearning-Lecture03.pdf'}\n",
      "{'page': 10, 'source': '../data/MachineLearning-Lecture03.pdf'}\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Additional tricks: compression\n",
    "\n",
    "Another approach for improving the quality of retrieved docs is compression.\n",
    "\n",
    "Information most relevant to a query may be buried in a document with a lot of irrelevant text. \n",
    "\n",
    "Passing that full document through your application can lead to more expensive LLM calls and poorer responses.\n",
    "\n",
    "Contextual compression is meant to fix this. \n",
    "\n",
    "Read more here: https://python.langchain.com/docs/modules/data_connection/retrievers/contextual_compression/#adding-contextual-compression-with-an-llmchainextractor"
   ],
   "id": "fabf3a0c4f6e192d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T06:36:05.048307Z",
     "start_time": "2024-04-11T06:36:05.046659Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor"
   ],
   "id": "b9e93b1565c7696d",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T06:36:05.050965Z",
     "start_time": "2024-04-11T06:36:05.049204Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Just making output a bit nicer\n",
    "def pretty_print_docs(docs):\n",
    "    print(f\"\\n{'-' * 100}\\n\".join([f\"Document {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]))"
   ],
   "id": "7f0ba70b5dab7522",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T06:36:05.053862Z",
     "start_time": "2024-04-11T06:36:05.051746Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Wrap our vectorstore\n",
    "#llm = OpenAI(temperature=0, model=\"gpt-3.5-turbo-instruct\")\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=vectordb.as_retriever()\n",
    ")\n"
   ],
   "id": "cebbacc6ba927bb2",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T06:36:11.267146Z",
     "start_time": "2024-04-11T06:36:05.054504Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "question = \"what did they say about matlab?\"\n",
    "compressed_docs = compression_retriever.get_relevant_documents(question)\n",
    "pretty_print_docs(compressed_docs)"
   ],
   "id": "39f9b3309918d703",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.11/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.11/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.11/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "\n",
      "MATLAB is I guess part of the programming language that makes it very easy to write codes using matrices, to write code for numerical routines, to move data around, to plot data. And it's sort of an extremely easy to learn tool to use for implementing a lot of learning algorithms.  \n",
      "there's also a software package called Octave that you can download for free off the Internet. And it has somewhat fewer features than MATLAB, but it's free, and for the purposes of this class, it will work for just about everything.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "\n",
      "MATLAB is I guess part of the programming language that makes it very easy to write codes using matrices, to write code for numerical routines, to move data around, to plot data. And it's sort of an extremely easy to learn tool to use for implementing a lot of learning algorithms.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3:\n",
      "\n",
      "\"Oh, it was the MATLAB.\"\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 4:\n",
      "\n",
      "\"Oh, it was the MATLAB.\"\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Combining various techniques\n",
    "\n",
    "Combining compression and MMR can lead to even better results."
   ],
   "id": "3311782db93dfba9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T06:36:11.270096Z",
     "start_time": "2024-04-11T06:36:11.268167Z"
    }
   },
   "cell_type": "code",
   "source": [
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=vectordb.as_retriever(search_type = \"mmr\")\n",
    ")"
   ],
   "id": "894bae0598c704d2",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T06:36:15.096073Z",
     "start_time": "2024-04-11T06:36:11.270746Z"
    }
   },
   "cell_type": "code",
   "source": [
    "question = \"what did they say about matlab?\"\n",
    "compressed_docs = compression_retriever.get_relevant_documents(question)\n",
    "pretty_print_docs(compressed_docs)"
   ],
   "id": "debd63ce613c8015",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.11/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.11/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.11/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "\n",
      "MATLAB is I guess part of the programming language that makes it very easy to write codes using matrices, to write code for numerical routines, to move data around, to plot data. And it's sort of an extremely easy to learn tool to use for implementing a lot of learning algorithms.  \n",
      "there's also a software package called Octave that you can download for free off the Internet. And it has somewhat fewer features than MATLAB, but it's free, and for the purposes of this class, it will work for just about everything.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "\n",
      "\"Oh, it was the MATLAB.\"\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T06:36:15.099303Z",
     "start_time": "2024-04-11T06:36:15.097130Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "69919d7078769511",
   "outputs": [],
   "execution_count": 26
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
